## Project Overview
The scope of the project changed slightly.  Rather than modeling any NBA player's contract, I wil be focusing on the first contract an NBA player receives after his rookie deal.  All rookie deals rookie deals are up to 4 years, so I will be focusing on the salary in the 5th season, using the player's data for the previous seasons to create the model.

## Data Wrangling Steps
I relied heavily on Basketball Reference to gather all the relevant data.  First, I had to get my list of drafted NBA players.  I used BeautifulSoup to get this information.  I iterated over the yearly Draft pages and scraped all the links to the individual players.  This created a list of links so that I could easily go to each player's page and grab the desired information.

Using the list I had just created, I navigate to each player's page and scrape the data I want, particularly the Per Game table, Advanced table, and the Salary table.  I wrote some functions to grab the data from each of these tables and put them in 3 separate dataframes.  Once I have all 3 dataframes, I merge them into 1 dataframe, as well as appending the player's name.  Lastly, I store this in a list of dataframes.

Once I have merged each set of 3 dataframes for every player in my list, I concatenated all the dataframes together to make one large dataframe.  Next, I exported this large dataframe into a tsv so I can access it more easily going forward.

## Data Cleaning Steps
During the data scraping process, I encountered many scenarios of bad data, or bad assumptions on my part, that I had to handle.  Most notably, not every player drafted in my data set made it to the NBA, so some did not have player pages.  Naturally, I'd want to skip these players.  I checked to see if they had a hyperlink embeded in their name on the draft page, if not, skip them and continue.
Similarly, some players had individual webpages, but not all the necessary info on their webpage.  I then wrote code to check if they had all the necessary info, if they did not, I would skip them and carry on.
Another issue I encountered was that about 20% of all the dataframes had a couple "hidden" columns in them that caused my merge functions to not work properly.  I had to track down these columns, find if they existed, and drop them if so.  This ended up being tricky because their names had some unicode characters in them which took a while to determine, and thus drop.